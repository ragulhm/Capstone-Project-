# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RPezGBxpvLMEChfivvB9KIvvogoa629o
"""

import torch
print("CUDA available:", torch.cuda.is_available())

!pip install -q transformers datasets scikit-learn pandas emoji nltk shap

import pandas as pd

df = pd.read_excel("Normalised_LLM_Lables.xlsx", engine="openpyxl")

print(df.head())
print(df.shape)

print(df.columns)

# keep only required columns
df = df[["text", "account.type"]]

# rename to project standard
df = df.rename(columns={
    "text": "tweet",
    "account.type": "label"
})

print(df.columns)
df.head()

df["label"] = df["label"].astype(int)
df["label"].value_counts()

import re
import emoji

def clean_tweet(text):
    text = str(text).lower()
    text = re.sub(r"http\S+|www\S+", "<url>", text)
    text = re.sub(r"@\w+", "<user>", text)
    text = re.sub(r"#\w+", "<hashtag>", text)
    text = re.sub(r"\d+", "<number>", text)
    text = emoji.replace_emoji(text, replace="<emoji>")
    text = re.sub(r"\s+", " ", text).strip()
    return text

df["clean_tweet"] = df["tweet"].apply(clean_tweet)
df.head()

texts = df["clean_tweet"].tolist()
labels = df["label"].tolist()

print(len(texts), len(labels))

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    texts,
    labels,
    test_size=0.2,
    random_state=42,
    stratify=labels
)

train_encodings = tokenizer(
    X_train,
    truncation=True,
    padding=True,
    max_length=60,
    return_tensors="pt"
)

test_encodings = tokenizer(
    X_test,
    truncation=True,
    padding=True,
    max_length=60,
    return_tensors="pt"
)

import torch

class TweetDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = TweetDataset(train_encodings, y_train)
test_dataset = TweetDataset(test_encodings, y_test)

print("Train samples:", len(train_dataset))
print("Test samples:", len(test_dataset))

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=5,
    weight_decay=0.01,
    logging_steps=100,
    report_to="none"
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

results = trainer.evaluate()
results

model.save_pretrained("bot_model")
tokenizer.save_pretrained("bot_model")

streamlit run app.py